{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis Lab\n",
    "### Completed by Jacob Metzger\n",
    "### Due Feb 15, 2016\n",
    "\n",
    "## Task Description\n",
    "Your assignment for this week is to do LSA on a group of newsgroup posts from the newsgroup 'rec.sport.baseball.'  (Feel free to pick another newsgroup if you like, the list is here.  http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)   \n",
    "\n",
    "1.  To get the newsgroup data, use this code:<br>\n",
    "from sklearn.datasets import fetch_20newsgroups<br>\n",
    "categories = ['rec.sport.baseball']<br>\n",
    "dataset = fetch_20newsgroups(subset='all',shuffle=True, random_state=42, categories=categories)<br>\n",
    "corpus = dataset.data<br>\n",
    "\n",
    "2.  Next, you'll be adapting my LSA code for your problem.  This shouldn't be too hard, but please spend some time understanding what my code is doing.  \n",
    "\n",
    "3.  When you print the discovered concepts you'll probably find they don't make sense.  Consider adjusting the words in the stop word list to remove things like nntp, and people's names...\n",
    "\n",
    "4.  Once youre satisfied with your work, submit the link to your work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Work\n",
    "#### (Note: Code in this notebook is adapted from course lecture sample code, some portions directly, found at https://github.com/mbernico/CS570/blob/master/LSA%20Text.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "categories = ['sci.space'] #Chosen newsgroup\n",
    "dataset = fetch_20newsgroups(subset='all',shuffle=True, random_state=42, categories=categories)\n",
    "corpus = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Not importing BeautifulSoup as data is not XML-like\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Izzy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "#Start some ad-hoc parsing to skip header and get body\n",
    "corpusSplit = [_.lower().split('\\n') for _ in corpus]\n",
    "dropHeaderCorpus = defaultdict(str)\n",
    "for idx, document in enumerate(corpusSplit):\n",
    "    for line in document:\n",
    "        if line.startswith(\"from:\"):\n",
    "            continue\n",
    "        elif line.startswith(\"subject:\"):\n",
    "            continue\n",
    "        elif line.startswith(\"organization:\"):\n",
    "            continue\n",
    "        elif line.startswith(\"distribution:\"):\n",
    "            continue\n",
    "        elif line.startswith(\"lines: \"):\n",
    "            continue\n",
    "        elif line.startswith(\"nntp-posting-host:\"):\n",
    "            continue\n",
    "        elif line.startswith(\"article-i.d.:\"):\n",
    "            continue\n",
    "        elif line.startswith(\"x-added:\"):\n",
    "            continue\n",
    "        elif line.startswith(\"original-sender:\"):\n",
    "            continue\n",
    "        dropHeaderCorpus[idx]+=' '+line.replace('<','').replace('>','') #also clean up some annoying symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopset = set(stopwords.words('english'))\n",
    "#These are ad-hoc as a result of trial and error\n",
    "stopset.update(['zoo','\\t','henry','com','toronto','re','edu','__','___', '_____',\\\n",
    "                'also','like','gmt', 'gov', 'net','theporch','raider','digex'])\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stopset, use_idf=True, ngram_range=(1,3))\n",
    "X=vectorizer.fit_transform(dropHeaderCorpus.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(987, 244327)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=8, n_iter=100,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use Single Value Decomposition to complete the LSA\n",
    "lsa = TruncatedSVD(n_components=8, n_iter=100) #n_components was chosen after trial and error based on inspection\n",
    "lsa.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "\tspace\n",
      "\twould\n",
      "\tnasa\n",
      "\twrites\n",
      "\tone\n",
      "\tarticle\n",
      "\tshuttle\n",
      "\tlaunch\n",
      "\torbit\n",
      "\tmoon\n",
      " \n",
      "Concept 1:\n",
      "\tspace\n",
      "\tvenus\n",
      "\tplanet\n",
      "\tmission\n",
      "\tsurface\n",
      "\tsolar\n",
      "\tspacecraft\n",
      "\tkilometers\n",
      "\tsolar system\n",
      "\tgood\n",
      " \n",
      "Concept 2:\n",
      "\thst\n",
      "\tmission\n",
      "\twork\n",
      "\teven\n",
      "\tsky\n",
      "\treal\n",
      "\tpeople\n",
      "\tpluto\n",
      "\tmaybe\n",
      "\tsomething\n",
      " \n",
      "Concept 3:\n",
      "\tnasa\n",
      "\tthink\n",
      "\tpat\n",
      "\teven\n",
      "\tspace\n",
      "\tsoftware\n",
      "\tmay\n",
      "\tshuttle\n",
      "\tlaunch\n",
      "\tmake\n",
      " \n",
      "Concept 4:\n",
      "\twould\n",
      "\tlaunch\n",
      "\tsatellite\n",
      "\tthings\n",
      "\tyear\n",
      "\tenough\n",
      "\tuse\n",
      "\tvehicle\n",
      "\tproject\n",
      "\tflight\n",
      " \n",
      "Concept 5:\n",
      "\tpeople\n",
      "\tlaunch\n",
      "\tnasa\n",
      "\tsolar\n",
      "\tsee\n",
      "\treply\n",
      "\tpropulsion\n",
      "\tthink\n",
      "\tfirst\n",
      "\thst\n",
      " \n",
      "Concept 6:\n",
      "\twould\n",
      "\tmission\n",
      "\taccess\n",
      "\tprogram\n",
      "\tsee\n",
      "\tproject\n",
      "\tcould\n",
      "\tprobe\n",
      "\tdc\n",
      "\tthings\n",
      " \n",
      "Concept 7:\n",
      "\tspace\n",
      "\tsee\n",
      "\talaska\n",
      "\tone\n",
      "\tus\n",
      "\tnew\n",
      "\tfirst\n",
      "\tday\n",
      "\tsolar\n",
      "\tlaunch\n",
      " \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_): \n",
    "    termsInComp = zip (terms,comp)\n",
    "    sortedTerms =  sorted(termsInComp, key=lambda x: x[1], reverse=True) [:10]\n",
    "    print \"Concept %d:\" % i\n",
    "    for term in sortedTerms:\n",
    "        print \"\\t\",term[0] #added indentation to make slightly easier to read\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
